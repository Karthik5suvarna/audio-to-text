# PyTorch — only needed for CUDA device detection; faster-whisper uses CTranslate2 internally
# Change the index URL to match your CUDA version:
#   CUDA 11.8: https://download.pytorch.org/whl/cu118
#   CUDA 12.1: https://download.pytorch.org/whl/cu121
#   CUDA 12.4: https://download.pytorch.org/whl/cu124
#   CPU only:  https://download.pytorch.org/whl/cpu
--extra-index-url https://download.pytorch.org/whl/cu121
torch>=2.2.0

# faster-whisper (CTranslate2 backend — much faster than HF transformers pipeline)
faster-whisper>=1.1.0

# FastAPI stack
fastapi>=0.115.0
uvicorn[standard]>=0.34.0
python-multipart>=0.0.18
httpx>=0.27.0

# Async file I/O
aiofiles>=24.1.0

# Config
pydantic>=2.0.0
pydantic-settings>=2.0.0

# SSE streaming
sse-starlette>=2.0.0
